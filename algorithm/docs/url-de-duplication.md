# 网页爬虫中的URL去重
 
## 散列表
拿散列表来举例。假设我们要爬取 10 亿个网页，假设一个 URL 的平均长度是 64 字节，那单纯存储这 10 亿个 URL，需要大约 60GB 的内存空间。
>q:散列表中添加、查找数据的时间复杂度已经是 O(1)，还能有进一步优化的空间吗？
> 
> a:时间复杂度并不能完全代表代码的执行时间。大 O 时间复杂度表示法，会忽略掉常数、系数和低阶，
> 并且统计的对象是语句的频度。不同的语句，执行时间也是不同的。时间复杂度只是表示执行时间随数据规模的变化趋势，
> 并不能度量在特定的数据规模下，代码执行时间的多少。

如果我们用基于链表的方法解决冲突问题，散列表中存储的是 URL，那当查询的时候，通过哈希函数定位到某个链表之后，
我们还需要依次比对每个链表中的 URL。这个操作是比较耗时的
* 链表中的结点在内存中不是连续存储的，所以不能一下子加载到 CPU 缓存中，没法很好地利用到 CPU 高速缓存，所以数据访问性能方面会打折扣。
* 而 URL 不是简单的数字，是平均长度为 64 字节的字符串。也就是说，我们要让待判重的 URL，跟链表中的每个 URL，做字符串匹配。 
比起单纯的数字比对，要慢很多。
  
## 位图
如果用散列表存储这 1 千万的数据，数据是 32 位的整型数，也就是需要 4 个字节的存储空间，那总共至少需要 40MB 的存储空间。
如果我们通过位图的话，数字范围在 1 到 1 亿之间，只需要 1 亿个二进制位，也就是 12MB 左右的存储空间就够了。
如果数字范围不是 1 到 1 亿，而是 1 到 10 亿，那位图的大小就是 10 亿个二进制位，也就是 120MB 的大小，消耗的内存空间，不降反增。

## 布隆过滤器
数据个数是 1 千万，数据的范围是 1 到 10 亿。布隆过滤器的做法是，我们仍然使用一个 1 亿个二进制大小的位图，然后通过哈希函数，
对数字进行处理，让它落在这 1 到 1 亿范围内。比如我们把哈希函数设计成 f(x)=x%n。其中，x 表示数字，n 表示位图的大小（1 亿），
也就是，对数字跟位图的大小进行取模求余。

使用 K 个哈希函数，对同一个数字进行求哈希值，那会得到 K 个不同的哈希值，用 K 个二进制位，来表示一个数字的存在。

### 特点
布隆过滤器的误判有一个特点，那就是，它只会对存在的情况有误判。

### 适合场景
适合对误判有一定的容忍度的场景。
* 爬虫判重
* UV 数

>q:利用布隆过滤器，在执行效率方面，是否比散列表更加高效呢？
> 
> a:布隆过滤器用多个哈希函数对同一个网页链接进行处理，CPU 只需要将网页链接从内存中读取一次，进行多次哈希计算，
> 理论上讲这组操作是 CPU 密集型的。而在散列表的处理方式中，需要读取散列值相同（散列冲突）的多个网页链接，
> 分别跟待判重的网页链接，进行字符串匹配。这个操作涉及很多内存数据的读取，所以是内存密集型的。
> 我们知道 CPU 计算可能是要比内存访问更快速的，所以，理论上讲，布隆过滤器的判重方式，更加快速。

## 参考
* [**数据结构与算法之美**](http://gk.link/a/10p9l)

  
